{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/CAP4770-group9-project/final-project/blob/notebook%2Fkmeans/k-means-clustering.ipynb",
      "authorship_tag": "ABX9TyNR6Wkg2Nq6EiXUGjo8IK7N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CAP4770-group9-project/final-project/blob/notebook%2Fkmeans/k-means-clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFQJfS-88urd",
        "colab_type": "text"
      },
      "source": [
        "# K-Means Clustering for Stock Portfolio Diversification\n",
        "\n",
        "Diversification is a very important step in stock portfolio creation. Picking several stocks rather than just one helps to reduce some of the inherent risk with investing. However, to reap the full benefits of diversification one not only has to pick more than one stock but also ensure that the behaviour of the stocks are not correlated.\n",
        "\n",
        "This measure of \"differentiality\" in stock return and variance behavior can be calculated using K means clustering. The stock price data over diffrent periods of time can be put into the clustering algorithm with the goal of clustering the stock based on diffrent behavior. Stocks from these clusters then can be chosen in order to create a well-diversified portfolio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_kpggokog9G",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnXKxqiBsIc5",
        "colab_type": "text"
      },
      "source": [
        "### Import data and calculate parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVSM7-grCVje",
        "colab_type": "text"
      },
      "source": [
        "Now we can import the S&P500 data into Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKCEaZB1DCiK",
        "colab_type": "code",
        "outputId": "7bd39f67-682a-4d3b-d89e-074acd02fbd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#change directory to where the data is located \n",
        "!pwd\n",
        "%cd drive/My Drive/cap4770-project"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/cap4770-project\n",
            "[Errno 2] No such file or directory: 'drive/My Drive/cap4770-project'\n",
            "/content/drive/My Drive/cap4770-project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qey89DoCCTzM",
        "colab_type": "code",
        "outputId": "43e3d55e-8c32-4db6-c8b6-a8597dcd2380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "#use timer to get process times\n",
        "start_time = time.time()\n",
        "\n",
        "#read csv\n",
        "data = pd.read_csv(\"SP_DAILY_2000-2020.csv\")\n",
        "print(\"read data --- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "availableStocks = data['Symbol'].unique()\n",
        "print(\"Number of stocks in the dataset: \", len(availableStocks))\n",
        "print(data)"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read data --- 1.6911883354187012 seconds ---\n",
            "Number of stocks in the dataset:  503\n",
            "        Symbol        Date    Open   Close     High     Low    Volume\n",
            "0          MMM  2020-03-26  131.79  135.98  136.355  130.61   5476351\n",
            "1          MMM  2020-03-25  133.15  131.54  134.690  126.80   7732467\n",
            "2          MMM  2020-03-24  122.29  132.72  133.450  121.00   9304832\n",
            "3          MMM  2020-03-23  128.16  117.87  128.400  114.04   7920348\n",
            "4          MMM  2020-03-20  138.07  124.89  139.240  122.71   9582251\n",
            "...        ...         ...     ...     ...      ...     ...       ...\n",
            "2250258    ZTS  2013-02-07   31.00   32.00   32.730   31.00   3800800\n",
            "2250259    ZTS  2013-02-06   30.98   31.03   31.430   30.75   2126100\n",
            "2250260    ZTS  2013-02-05   31.25   31.04   31.980   30.85   5013200\n",
            "2250261    ZTS  2013-02-04   31.09   31.02   31.990   30.76   7695400\n",
            "2250262    ZTS  2013-02-01   31.50   31.01   31.740   30.47  66789100\n",
            "\n",
            "[2250263 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqtI8wO8nu5p",
        "colab_type": "text"
      },
      "source": [
        "Now we can extract the training data from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmfo9Kvyp_Dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create DateTime column\n",
        "stocksData = data\n",
        "\n",
        "stocksData['DateTime'] = pd.to_datetime(stocksData.apply(lambda row: row['Date'], axis=1))\n",
        "\n",
        "print(stocksData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cvgeDWkpVih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get full training data set\n",
        "trainData = stocksData[(stocksData['DateTime'] > '2010-03-26') & (stocksData['DateTime'] < '2015-03-26')]\n",
        "\n",
        "#get training data set for 2015\n",
        "trainData2015 = stocksData[(stocksData['DateTime'] > '2014-03-26') & (stocksData['DateTime'] < '2015-03-26')]\n",
        "print(trainData2015)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3tqidRttemt",
        "colab_type": "text"
      },
      "source": [
        "We can perform an initial visualization of the data by plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DI21idmtxI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb978D6vty9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grouped_day_stocks = []\n",
        "grouped_day = []\n",
        "\n",
        "#plot all data\n",
        "#plt.figure()\n",
        "#for i in range(0, len(availableStocks)):\n",
        "#  grouped_day_stocks.append(stocksData[stocksData['Symbol'] == availableStocks[i]])\n",
        "#  grouped_day.append(grouped_day_stocks[i].groupby(lambda x: grouped_day_stocks[i]['DateTime'][x]))\n",
        "#for j in range(0, len(availableStocks)):\n",
        "#  plt.plot(grouped_day[j].size().index, grouped_day_stocks[j]['Close'])\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtfwbKch6MRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#group day for all training data\n",
        "grouped_day_stocks = []\n",
        "grouped_day = []\n",
        "\n",
        "for i in range(0, len(availableStocks)):\n",
        "  grouped_day_stocks.append(trainData[trainData['Symbol'] == availableStocks[i]])\n",
        "  grouped_day.append(grouped_day_stocks[i].groupby(lambda x: grouped_day_stocks[i]['DateTime'][x]))\n",
        "\n",
        "#print(\"grouped day stocks\")\n",
        "#print(grouped_day_stocks, \"\\n\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYZaCOJNLFiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(grouped_day_stocks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMsHG4KNJ_yF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot only training data\n",
        "#plt.figure()\n",
        "#for j in range(0, len(availableStocks)):\n",
        "#  plt.plot(grouped_day[j].size().index, grouped_day_stocks[j]['Close'])\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjklINq1NZvd",
        "colab_type": "text"
      },
      "source": [
        "For this 1 year sample of the training data, we can calculate the annualized average returns (based on price at close), annualized average volatility (based on price at close), the average intraday price variation (pct change between daily high and low), average diffrence in price between open and close and annualized trading volume from the data. \n",
        "\n",
        "The intraday price variation and diffrence between open and close will be used as additional measures of volatility and hopefully will add richness to the resutls of the k-means analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msffkLwEMRnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#group day for 2015 data\n",
        "grouped_day_stocks = []\n",
        "grouped_day = []\n",
        "\n",
        "for i in range(0, len(availableStocks)):\n",
        "  grouped_day_stocks.append(trainData2015[trainData2015['Symbol'] == availableStocks[i]])\n",
        "  grouped_day.append(grouped_day_stocks[i].groupby(lambda x: grouped_day_stocks[i]['DateTime'][x]))\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmufRPOEMyFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(grouped_day_stocks))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PsSsQOAjxW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot performance of the SP500 over training period\n",
        "\n",
        "!pip install ffn\n",
        "import ffn\n",
        "prices = ffn.get('^GSPC',start='2014-03-26', end='2015-03-26')\n",
        "ax1 = prices.plot(title=\"Performance of S&P500 from 2014-2015\", figsize=(15, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA_sNF5rksro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot index overall returns\n",
        "returns = prices.to_returns().dropna()\n",
        "print(returns)\n",
        "print(returns.mean() *252)\n",
        "ax = returns.hist(figsize=(10,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VcEF6CkfMkXl",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Need to rebase price data in order to get a clearer visualization\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#plot only training data 2015\n",
        "#plt.figure()\n",
        "#for j in range(0, len(availableStocks)):\n",
        "#  plt.plot(grouped_day[j].size().index, grouped_day_stocks[j]['Close'])\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRDimSWeVvWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annualized_data = pd.DataFrame(index=availableStocks, columns=['Returns', 'Volatility','Volume', 'HighLowDiff', 'OpenCloseDiff'])\n",
        "\n",
        "\"\"\"\n",
        "Returns seem to be off a bit, as the sp500 itself has positive average returns but the returns data is negative\n",
        "\n",
        "\"\"\"\n",
        "#aggregate annualized stock data used in cluserting analysis into dataframe\n",
        "for i in range(0, len(grouped_day_stocks)):\n",
        "  \n",
        "  annualized_data.loc[availableStocks[i]]['Returns'] = grouped_day_stocks[i]['Close'].pct_change().mean() * 252\n",
        "  annualized_data.loc[availableStocks[i]]['Volatility'] = grouped_day_stocks[i]['Close'].pct_change().std() * 252\n",
        "  annualized_data.loc[availableStocks[i]]['Volume'] = grouped_day_stocks[i]['Volume'].mean() * 252\n",
        "  annualized_data.loc[availableStocks[i]]['HighLowDiff'] = ((grouped_day_stocks[i]['High'] - grouped_day_stocks[i]['Low']) / grouped_day_stocks[i]['Low']).mean()\n",
        "  annualized_data.loc[availableStocks[i]]['OpenCloseDiff'] = ((grouped_day_stocks[i]['Close'] - grouped_day_stocks[i]['Open']) / grouped_day_stocks[i]['Open']).mean()  \n",
        "\n",
        "print(\"Annualized data for each stock in S&P500 calculated form 2014 to 2015:\")\n",
        "print(annualized_data)\n",
        "\n",
        "print(\"Min/Max statistics: \\n\")\n",
        "print(\"Max returns: \")\n",
        "print(annualized_data[annualized_data['Returns'] == max(annualized_data['Returns'])], \"\\n\")\n",
        "print(\"Min returns: \")\n",
        "print(annualized_data[annualized_data['Returns'] == min(annualized_data['Returns'])], \"\\n\")\n",
        "\n",
        "print(\"Max volatility: \")\n",
        "print(annualized_data[annualized_data['Volatility'] == max(annualized_data['Volatility'],key=abs)], \"\\n\")\n",
        "print(\"Min volatility: \")\n",
        "print(annualized_data[annualized_data['Volatility'] == min(annualized_data['Volatility'],key=abs)], \"\\n\")\n",
        "\n",
        "print(\"Max volume: \")\n",
        "print(annualized_data[annualized_data['Volume'] == max(annualized_data['Volume'],key=abs)], \"\\n\")\n",
        "print(\"Min volume: \")\n",
        "print(annualized_data[annualized_data['Volume'] == min(annualized_data['Volume'],key=abs)], \"\\n\")\n",
        "\n",
        "print(\"Max HighLowDiff: \")\n",
        "print(annualized_data[annualized_data['HighLowDiff'] == max(annualized_data['HighLowDiff'],key=abs)], \"\\n\")\n",
        "print(\"Min HighLowDiff: \")\n",
        "print(annualized_data[annualized_data['HighLowDiff'] == min(annualized_data['HighLowDiff'],key=abs)], \"\\n\")\n",
        "\n",
        "print(\"Max OpenCloseDiff: \")\n",
        "print(annualized_data[annualized_data['OpenCloseDiff'] == max(annualized_data['OpenCloseDiff'],key=abs)], \"\\n\")\n",
        "print(\"Min OpenCloseDiff: \")\n",
        "print(annualized_data[annualized_data['OpenCloseDiff'] == min(annualized_data['OpenCloseDiff'],key=abs)], \"\\n\")\n",
        "\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WFLPFEQjTkV",
        "colab_type": "text"
      },
      "source": [
        "###Plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lld4Bopxm_VC",
        "colab_type": "text"
      },
      "source": [
        "####4D Plot | Returns, Volatility, Volume, OpenCloseDiff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABzeQuE3nf8Q",
        "colab_type": "text"
      },
      "source": [
        "We will now attempt to plot the returns, volatility, volume and openclose diff attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5AR39jkoLnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Concatenating the returns and variances and volume into a single data-frame\n",
        "ret_var_vol_ocDiff = annualized_data.drop(['HighLowDiff'], axis=1)\n",
        "print(ret_var_vol_ocDiff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtMJkwo_nfN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#the below graph plots returns, volatility and volume on the axes and uses a heat map to display OpenCloseDiff\n",
        "fig = plt.figure(figsize=(15,15))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "fig.suptitle('Annualized S&P 500 Data 2014-2015', fontsize=16)\n",
        "\n",
        "x = list(ret_var_vol_ocDiff['Returns'].dropna().values)\n",
        "y = list(ret_var_vol_ocDiff['Volatility'].dropna().values)\n",
        "z = list(ret_var_vol_ocDiff['Volume'].dropna().values)\n",
        "c = list(ret_var_vol_ocDiff['OpenCloseDiff'].dropna().values)\n",
        "\n",
        "img = ax.scatter(x, y, z, c=c, cmap=plt.hot())\n",
        "fig.colorbar(img)\n",
        "ax.set_xlabel('Returns')\n",
        "ax.set_ylabel('Volatility')\n",
        "ax.set_zlabel('Volume')\n",
        "ax.set_xlim(-1,1)\n",
        "ax.set_zlim(0,.5e10)\n",
        "ax.set_ylim(0,25)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZBG2S6RmLxd",
        "colab_type": "text"
      },
      "source": [
        "For now we will run the K means using only 3 attributes, the mean returns, variance, and volume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThH5919Tmcnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Concatenating the returns and variances and volume into a single data-frame\n",
        "ret_var_vol = annualized_data.drop(['HighLowDiff','OpenCloseDiff'], axis=1)\n",
        "print(ret_var_vol)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL-_YSoQnIFX",
        "colab_type": "text"
      },
      "source": [
        "####3D Plot | Returns, Volatility, Volume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHzZEKwrtGdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "#first plot data to spot outliers\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "x = list(ret_var_vol['Returns'].dropna().values)\n",
        "y = list(ret_var_vol['Volatility'].dropna().values)\n",
        "z = list(ret_var_vol['Volume'].dropna().values)\n",
        "#print(x, \"\\n\", len(x))\n",
        "#print(y, \"\\n\", len(y))\n",
        "#print(z, \"\\n\", len(z))\n",
        "\n",
        "ax.scatter(x, y, z, c='r', marker='o')\n",
        "\n",
        "ax.set_xlabel('Returns')\n",
        "ax.set_ylabel('Volatility')\n",
        "ax.set_zlabel('Volume')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOytvtclESwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#identify the volatility outlier, drop, then graph again\n",
        "#2014-2015 volatility outliers 'AAPL', 'V', 'HBI'\n",
        "#outlier = ret_var_vol['Volatility'] == max(ret_var_vol['Volatility'],key=abs)\n",
        "#outlierIndex = ret_var_vol[outlier].index\n",
        "#print(outlierIndex, \"\\n\")\n",
        "\n",
        "outlierIndex = ['AAPL','V','HBI']\n",
        "ret_var_vol.drop(outlierIndex, inplace =True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwrH6ZTjETSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code to drop volume outliers\n",
        "#2014 to 2015 volume outlier 'BAC'\n",
        "#outlier = ret_var_vol['Volume'] == max(ret_var_vol['Volume'],key=abs)\n",
        "#outlierIndex = ret_var_vol[outlier].index\n",
        "#print(outlierIndex, \"\\n\")\n",
        "\n",
        "outlierIndex = 'BAC'\n",
        "ret_var_vol.drop(outlierIndex, inplace =True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC7LMOoRE3sD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4787Eoc1hp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#regraph after outliers are dropped\n",
        "\n",
        "fig = plt.figure(figsize=(15,15))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "fig.suptitle('Annualized S&P 500 Data 2014-2015', fontsize=16)\n",
        "\n",
        "x = list(ret_var_vol['Returns'].dropna().values)\n",
        "y = list(ret_var_vol['Volatility'].dropna().values)\n",
        "z = list(ret_var_vol['Volume'].dropna().values)\n",
        "#print(x, \"\\n\", len(x))\n",
        "#print(y, \"\\n\", len(y))\n",
        "#print(z, \"\\n\", len(z))\n",
        "\n",
        "ax.scatter(x, y, z, c='r', marker='o')\n",
        "#ax.title(\"Annualized S&P500 Data\")\n",
        "\n",
        "ax.set_xlabel('Returns')\n",
        "ax.set_ylabel('Volatility')\n",
        "ax.set_zlabel('Volume')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_L_eRbKxERK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68uET-Srw9tD",
        "colab_type": "text"
      },
      "source": [
        "##Using PCA to Reduce Dimensionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBnMs7q2xHDi",
        "colab_type": "text"
      },
      "source": [
        "The dimensionality of the data chosen for the analysis below is arbitrary. The most common measures of a stock's performance in portfolio optimization analysis are mean and variance. However, since the data obtained from the AlphaVantage API includes additional dimensionality, it is worth exploring if the additional dimensions would add richness to our analysis.\n",
        "\n",
        "Before we perform our clustering analysis, we can determine the most prominent features in the data using a Principal Component Analysis. This will be used to reduce the 5 dimensional data into a smaller amount of principal components. The clustering analysis can then be performed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB_v_miRB-nx",
        "colab_type": "text"
      },
      "source": [
        "First the data needs to be standardized as PCA is affected by scale. Normalizing will improve the maximization of the variance for each component that is needed in PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCB9481LCCuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
        "#https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#use imputer to remobe NaN values and replace with mean along each column\n",
        "imp = SimpleImputer(strategy=\"mean\")\n",
        "scale = StandardScaler()\n",
        "print(annualized_data)\n",
        "\n",
        "#use scaler to normalize data before clustering\n",
        "annualized_data_scaled = scale.fit_transform(imp.fit_transform(annualized_data.values))\n",
        "print(annualized_data_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crg-rPl5DMOm",
        "colab_type": "text"
      },
      "source": [
        "Now we can calculate the explained variance for each attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YoBFNI8DobB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fitting the PCA algorithm with our Data\n",
        "pca = PCA().fit(annualized_data_scaled)\n",
        "\n",
        "#Plotting the Cumulative Summation of the Explained Variance\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Variance (%)') #for each component\n",
        "plt.title('Stock Dataset Explained Variance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJZA0wghEYK-",
        "colab_type": "text"
      },
      "source": [
        "This indicates that there are only 3 components the indicate almost 100% of the variance in the model, therefore implying that there are only 3 principal components that need to be calculated. We can now proceed to perform the PCA analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEBUmDvqFF-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3)\n",
        "principalComponents = pca.fit_transform(annualized_data_scaled)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
        "print(principalDf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AIxuBLvNSuJ",
        "colab_type": "text"
      },
      "source": [
        "Now we can concatenate the tickers onto the principal components dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_3LSSy4Nole",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tickers = pd.DataFrame(annualized_data.index, columns=['Tickers'])\n",
        "labeled_pca_df = pd.concat([tickers, principalDf],axis = 1).dropna()\n",
        "print(labeled_pca_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph7F_FSJOjWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop outliers\n",
        "#outliers seem to have high principal component 3 value\n",
        "#outlier = labeled_PCA_df['principal component 3'] == max(labeled_PCA_df['principal component 3'],key=abs)\n",
        "#outlierTicker = labeled_PCA_df[outlier]['Tickers']\n",
        "#outlierIndex = labeled_PCA_df[outlier].index\n",
        "#print(outlierTicker)\n",
        "#print(outlierIndex, \"\\n\")\n",
        "\n",
        "outlierTickers = ['AAPL', 'BAC', 'HBI', 'V']\n",
        "print(labeled_pca_df)\n",
        "outlierIndex = labeled_pca_df[labeled_pca_df['Tickers'].isin(outlierTickers)].index\n",
        "labeled_pca_df.drop(outlierIndex, inplace =True)\n",
        "print(labeled_pca_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BlNnHCrLimy",
        "colab_type": "text"
      },
      "source": [
        "Now we can plot the principal components in principal component space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35b97A4OLp7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,15))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('Principal Component 3', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_zlabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('3 component PCA', fontsize = 20)\n",
        "\n",
        "x = list(labeled_pca_df['principal component 3'].dropna().values)\n",
        "y = list(labeled_pca_df['principal component 1'].dropna().values)\n",
        "z = list(labeled_pca_df['principal component 2'].dropna().values)\n",
        "\n",
        "ax.scatter(x, y, z, c='r', marker='o')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEd1MceYN_v3",
        "colab_type": "text"
      },
      "source": [
        "## Clustering Analysis | PCA Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD3Zt6WUOQGn",
        "colab_type": "text"
      },
      "source": [
        "Now that the data has been converted from a 5d to a 3d space, we can perform the clustering analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2izO0KTaO_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pylab import plot,show\n",
        "from numpy import vstack,array\n",
        "from numpy.random import rand\n",
        "import numpy as np\n",
        "from scipy.cluster.vq import kmeans,vq\n",
        "import pandas as pd\n",
        "import pandas_datareader as dr\n",
        "from math import sqrt\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYD__stTfqo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = labeled_pca_df.drop('Tickers', axis=1).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djXjupSeaQ3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate elbow curve\n",
        "distorsions = []\n",
        "for k in range(2, 20):\n",
        "    k_means = KMeans(n_clusters=k)\n",
        "    k_means.fit(X)\n",
        "    distorsions.append(k_means.inertia_)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "plt.plot(range(2, 20), distorsions)\n",
        "plt.grid(True)\n",
        "plt.title('Elbow curve')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5_3xoprafcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  pylab as pl\n",
        "\n",
        "#compute K means with n = 8 and get centroids\n",
        "kmeans = KMeans(n_clusters = 8)\n",
        "kmeans.fit(X)\n",
        "centroids = kmeans.cluster_centers_\n",
        "#print(centroids)\n",
        "#print(kmeans.labels_)\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('Principal Component 3', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_zlabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('K means clustering (n=8) on 3 component PCA Stock Data', fontsize = 20)\n",
        "\n",
        "x = list(labeled_pca_df['principal component 3'].dropna().values)\n",
        "y = list(labeled_pca_df['principal component 1'].dropna().values)\n",
        "z = list(labeled_pca_df['principal component 2'].dropna().values)\n",
        "\n",
        "\n",
        "ax.scatter(x, y, z, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,1],centroids[:,2],'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('Principal Component 3')\n",
        "ax.set_ylabel('Principal Component 1')\n",
        "ax.set_zlabel('Principal Component 2')\n",
        "ax.set_xlim(-6,4)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7M3mWt7eYBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,15))\n",
        "fig.suptitle('K-Means(n=8) Clustered PCA, 3 vs. 1', fontsize=16)\n",
        "ax = fig.add_subplot(2, 1, 1)\n",
        "ax.scatter(x, y, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,1], 'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('PCA 3')\n",
        "ax.set_ylabel('PCA 1')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-dSSeEgPrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,15))\n",
        "fig.suptitle('K-Means(n=8) Clustered PCA, 3 vs. 2', fontsize=16)\n",
        "ax = fig.add_subplot(2, 1, 1)\n",
        "ax.scatter(z, x, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,1], 'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('PCA 2')\n",
        "ax.set_ylabel('PCA 3')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyyploHxhlwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,15))\n",
        "fig.suptitle('K-Means(n=8) Clustered PCA, 2 vs. 1', fontsize=16)\n",
        "ax = fig.add_subplot(2, 1, 1)\n",
        "ax.scatter(y, z, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,1], 'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('PCA 1')\n",
        "ax.set_ylabel('PCA 2')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6LGZt0A3yG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_labels = pd.DataFrame(kmeans.labels_,columns=['Cluster'])\n",
        "labeled_stocks_pca = cluster_labels.set_index(labeled_pca_df['Tickers'])\n",
        "print(labeled_stocks_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gji4JT_9Ly-D",
        "colab_type": "text"
      },
      "source": [
        "##Clustering Analysis | Returns, Volume, Volatility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKbL1AP9Lua7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#taken from: https://towardsdatascience.com/machine-learning-for-stock-clustering-using-k-means-algorithm-126bc1ace4e1\n",
        "#https://www.pythonforfinance.net/2018/02/08/stock-clusters-using-k-means-algorithm-in-python/\n",
        "\n",
        "from pylab import plot,show\n",
        "from numpy import vstack,array\n",
        "from numpy.random import rand\n",
        "import numpy as np\n",
        "from scipy.cluster.vq import kmeans,vq\n",
        "import pandas as pd\n",
        "import pandas_datareader as dr\n",
        "from math import sqrt\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrHavKVoZap0",
        "colab_type": "text"
      },
      "source": [
        "In order to ensure accurate clustering results, we can now scale the variance and volume data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F0y2eQGZZzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#use imputer to remobe NaN values and replace with mean along each column\n",
        "imp = SimpleImputer(strategy=\"mean\")\n",
        "scale = StandardScaler()\n",
        "print(ret_var_vol)\n",
        "unscaled_ret_var_vol = ret_var_vol\n",
        "#use scaler to normalize data before clustering\n",
        "X = scale.fit_transform(imp.fit_transform(ret_var_vol.values))\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seoMoo1QbWSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ret_var_vol = pd.DataFrame(data = X\n",
        "             , columns = ['Returns', 'Volatility', 'Volume'])\n",
        "print(ret_var_vol)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1kHRVMQBROH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate elbow curve\n",
        "distorsions = []\n",
        "for k in range(2, 20):\n",
        "    k_means = KMeans(n_clusters=k)\n",
        "    k_means.fit(X)\n",
        "    distorsions.append(k_means.inertia_)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "plt.plot(range(2, 20), distorsions)\n",
        "plt.grid(True)\n",
        "plt.title('Elbow curve')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7YRWeze8SFo",
        "colab_type": "text"
      },
      "source": [
        "Now we can undertake in the K-means clustering and plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ujfng646psM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  pylab as pl\n",
        "\n",
        "#compute K means with n = 8 and get centroids\n",
        "kmeans = KMeans(n_clusters = 8)\n",
        "kmeans.fit(X)\n",
        "centroids = kmeans.cluster_centers_\n",
        "#print(centroids)\n",
        "#print(kmeans.labels_)\n",
        "\n",
        "fig = plt.figure(figsize=(15,15))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "fig.suptitle('K-Means(n=5) Clustered S&P 500 Data 2014-2015', fontsize=16)\n",
        "\n",
        "x = list(ret_var_vol['Returns'].values)\n",
        "y = list(ret_var_vol['Volatility'].values)\n",
        "z = list(ret_var_vol['Volume'].values)\n",
        "\n",
        "ax.scatter(x, y, z, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,1],centroids[:,2],'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('Returns')\n",
        "ax.set_ylabel('Volatility')\n",
        "ax.set_zlabel('Volume')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdaI63NS8bxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,15))\n",
        "fig.suptitle('K-Means(n=5) Clustered S&P 500 Data 2014-2015, Volatility vs. Returns', fontsize=16)\n",
        "ax = fig.add_subplot(2, 1, 1)\n",
        "ax.scatter(x, y, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,1], 'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('Returns')\n",
        "ax.set_ylabel('Volatility')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0oRNqI59YDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,15))\n",
        "fig.suptitle('K-Means(n=5) Clustered S&P 500 Data 2014-2015, Volatility vs. Volume', fontsize=16)\n",
        "ax = fig.add_subplot(2, 1, 1)\n",
        "ax.scatter(y, z, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,1],centroids[:,2], 'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('Volatility')\n",
        "ax.set_ylabel('Volume')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZJhNb_n9r5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,15))\n",
        "fig.suptitle('K-Means(n=5) Clustered S&P 500 Data 2014-2015, Returns vs. Volume', fontsize=16)\n",
        "ax = fig.add_subplot(2, 1, 1)\n",
        "ax.scatter(x, z, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,2], 'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('Returns')\n",
        "ax.set_ylabel('Volume')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjhIq9EyiEWz",
        "colab_type": "text"
      },
      "source": [
        "We try diffrent axes configurations to get additional perspectives of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iE3K41JiBIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,15))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "fig.suptitle('K-Means(n=5) Clustered S&P 500 Data 2014-2015', fontsize=16)\n",
        "\n",
        "ax.scatter(z, y, x, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,1],centroids[:,2],'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('Volume')\n",
        "ax.set_ylabel('Volatility')\n",
        "ax.set_zlabel('Returns')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoFOSd0-ikzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(15,15))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "fig.suptitle('K-Means(n=5) Clustered S&P 500 Data 2014-2015', fontsize=16)\n",
        "\n",
        "\n",
        "ax.scatter(z, x, y, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,1],centroids[:,2],'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('Volume')\n",
        "ax.set_ylabel('Returns')\n",
        "ax.set_zlabel('Volatility')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuaQ7B0UN0Cc",
        "colab_type": "text"
      },
      "source": [
        "Now that the clustering has been completed, we can create a new dataframe composed on the ticker names and their cluster labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5FyTkvoOEcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tickers = pd.DataFrame(ret_var_vol.index, columns=['Tickers'])\n",
        "cluster_labels = pd.DataFrame(kmeans.labels_,columns=['Cluster'])\n",
        "labeled_stocks_3d = pd.concat([tickers, cluster_labels],axis = 1).dropna()\n",
        "print(labeled_stocks_3d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7yEK6azEDIa",
        "colab_type": "text"
      },
      "source": [
        "## Clustering Analysis | Returns, Volatility\n",
        "\n",
        "A second clustering analysis can be done with just the returns and volatility parameters. The results of clustering on imporvement of diversification from this analysis and the three parameter analysis can be later compared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVuxeyM_EXf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate new dataframe with only two parameters\n",
        "print(ret_vol_var)\n",
        "ret_var = ret_vol_var.drop(['Volume'], axis=1)\n",
        "print(ret_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUdSLtaWnRYd",
        "colab_type": "text"
      },
      "source": [
        "####2D Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9NVdjtAFDmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#first plot data \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(15,15))\n",
        "ax = fig.add_subplot(2 ,1 ,1)\n",
        "fig.suptitle('S&P 500 Data 2014-2015', fontsize=16)\n",
        "\n",
        "x = list(ret_var['Returns'].dropna().values)\n",
        "y = list(ret_var['Volatility'].dropna().values)\n",
        "\n",
        "#print(x, \"\\n\", len(x))\n",
        "#print(y, \"\\n\", len(y))\n",
        "#print(z, \"\\n\", len(z))\n",
        "\n",
        "ax.scatter(x, y, c='r', marker='o')\n",
        "\n",
        "ax.set_xlabel('Returns')\n",
        "ax.set_ylabel('Volatility')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki1rxkQZGS4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#taken from: https://towardsdatascience.com/machine-learning-for-stock-clustering-using-k-means-algorithm-126bc1ace4e1\n",
        "#https://www.pythonforfinance.net/2018/02/08/stock-clusters-using-k-means-algorithm-in-python/\n",
        "\n",
        "from pylab import plot,show\n",
        "from numpy import vstack,array\n",
        "from numpy.random import rand\n",
        "import numpy as np\n",
        "from scipy.cluster.vq import kmeans,vq\n",
        "import pandas as pd\n",
        "import pandas_datareader as dr\n",
        "from math import sqrt\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajs40LkXe01s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "print(ret_var)\n",
        "#use imputer to remobe NaN values and replace with mean along each column\n",
        "imp = SimpleImputer(strategy=\"mean\")\n",
        "scale = StandardScaler()\n",
        "#use scaler to normalize data before clustering\n",
        "X = ret_var.values\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdokLuvcGcgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate elbow curve\n",
        "distorsions = []\n",
        "for k in range(2, 20):\n",
        "    k_means = KMeans(n_clusters=k)\n",
        "    k_means.fit(X)\n",
        "    distorsions.append(k_means.inertia_)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "plt.plot(range(2, 20), distorsions)\n",
        "plt.grid(True)\n",
        "plt.title('Elbow curve')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiZ0qupMG6M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  pylab as pl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#compute K means with n = 5 and get centroids\n",
        "kmeans = KMeans(n_clusters = 8)\n",
        "kmeans.fit(X)\n",
        "centroids = kmeans.cluster_centers_\n",
        "#print(centroids)\n",
        "#print(kmeans.labels_)\n",
        "\n",
        "fig = plt.figure(figsize=(15,15))\n",
        "ax = fig.add_subplot(2,1,1)\n",
        "fig.suptitle('K-Means(n=5) Clustered S&P 500 Data 2014-2015', fontsize=16)\n",
        "\n",
        "ax.scatter(x, y, c=k_means.labels_, marker='o')\n",
        "plot(centroids[:,0],centroids[:,1],'sb',markersize=8)\n",
        "ax.legend(['Centroids'])\n",
        "ax.set_xlabel('Returns')\n",
        "ax.set_ylabel('Volatility')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3cJh1_aHSPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tickers = pd.DataFrame(ret_var_vol.index, columns=['Tickers'])\n",
        "cluster_labels = pd.DataFrame(kmeans.labels_,columns=['Cluster'])\n",
        "labeled_stocks_2d = pd.concat([tickers, cluster_labels],axis = 1).dropna()\n",
        "print(labeled_stocks_2d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fusM2nMBHrGG",
        "colab_type": "text"
      },
      "source": [
        "Now we can compare the cluster labeling between the two analyses to see if adding volume as an additional parameter had a signifigant chnage on the clustering results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muGBQEc1H_82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(labeled_stocks_2d)\n",
        "#print(labeled_stocks_3d)\n",
        "print(labeled_stocks_2d[labeled_stocks_2d['Cluster'] != labeled_stocks_3d['Cluster']])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}